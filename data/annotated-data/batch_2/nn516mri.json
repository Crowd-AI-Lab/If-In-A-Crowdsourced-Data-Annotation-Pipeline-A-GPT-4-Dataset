{
    "paper_id": "3d587b2cba9d0af266fed4c7b7312aac555632d8",
    "cord_uid": "nn516mri",
    "sentence_segment": {
        "1": {
            "segment": "Pretrained protein sequence language models largely rely on the transformer architecture .",
            "label": {
                "basic": {
                    "B70": "background",
                    "B105": "purpose",
                    "B99": "background",
                    "B15": "method",
                    "B17": "purpose",
                    "B10": "background",
                    "B43": "purpose",
                    "B95": "finding",
                    "B18": "finding",
                    "B58": "background",
                    "B4": "method",
                    "B98": "purpose",
                    "B35": "finding",
                    "B20": "method",
                    "B47": "background",
                    "B51": "background",
                    "B107": "background",
                    "B100": "background",
                    "B2": "purpose",
                    "B104": "background"
                },
                "advanced": {
                    "A87": "background",
                    "A95": "method",
                    "A90": "background",
                    "A22": "purpose",
                    "A39": "background",
                    "A56": "background",
                    "A92": "purpose",
                    "A96": "background",
                    "A24": "background",
                    "A4": "background",
                    "A91": "method",
                    "A2": "background",
                    "A18": "purpose",
                    "A35": "background",
                    "A101": "other",
                    "A7": "other",
                    "A17": "purpose",
                    "A6": "background",
                    "A27": "background",
                    "A40": "background"
                },
                "gpt-4": {
                    "gpt-t1.0": "background",
                    "gpt-t0.2": "background"
                },
                "cs-expert": "background",
                "bio-expert": "background"
            }
        },
        "2": {
            "segment": "However , transformer run-time and memory requirements scale quadrat-ically with sequence length .",
            "label": {
                "basic": {
                    "B70": "purpose",
                    "B105": "method",
                    "B99": "purpose",
                    "B15": "background",
                    "B17": "method",
                    "B10": "finding",
                    "B43": "background",
                    "B95": "method",
                    "B18": "finding",
                    "B58": "purpose",
                    "B4": "background",
                    "B98": "purpose",
                    "B35": "finding",
                    "B20": "other",
                    "B47": "background",
                    "B51": "background",
                    "B107": "method",
                    "B100": "background",
                    "B2": "method",
                    "B104": "purpose"
                },
                "advanced": {
                    "A87": "purpose",
                    "A95": "background",
                    "A90": "background",
                    "A22": "method",
                    "A39": "purpose",
                    "A56": "purpose",
                    "A92": "method",
                    "A96": "purpose",
                    "A24": "purpose",
                    "A4": "purpose",
                    "A91": "finding",
                    "A2": "finding",
                    "A18": "method",
                    "A35": "purpose",
                    "A101": "method",
                    "A7": "background",
                    "A17": "method",
                    "A6": "background",
                    "A27": "purpose",
                    "A40": "purpose"
                },
                "gpt-4": {
                    "gpt-t1.0": "background",
                    "gpt-t0.2": "background"
                },
                "cs-expert": "background",
                "bio-expert": "purpose"
            }
        },
        "3": {
            "segment": "We investigate the potential of a convolution-based architecture for protein sequence masked language model pretraining and subsequent finetuning .",
            "label": {
                "basic": {
                    "B70": "method",
                    "B105": "finding",
                    "B99": "purpose",
                    "B15": "method",
                    "B17": "background",
                    "B10": "method",
                    "B43": "method",
                    "B95": "purpose",
                    "B18": "finding",
                    "B58": "purpose",
                    "B4": "finding",
                    "B98": "method",
                    "B35": "finding",
                    "B20": "finding",
                    "B47": "purpose",
                    "B51": "method",
                    "B107": "purpose",
                    "B100": "purpose",
                    "B2": "background",
                    "B104": "finding"
                },
                "advanced": {
                    "A87": "purpose",
                    "A95": "finding",
                    "A90": "purpose",
                    "A22": "finding",
                    "A39": "method",
                    "A56": "method",
                    "A92": "finding",
                    "A96": "finding",
                    "A24": "method",
                    "A4": "method",
                    "A91": "method",
                    "A2": "method",
                    "A18": "finding",
                    "A35": "other",
                    "A101": "purpose",
                    "A7": "other",
                    "A17": "background",
                    "A6": "background",
                    "A27": "background",
                    "A40": "method"
                },
                "gpt-4": {
                    "gpt-t1.0": "purpose",
                    "gpt-t0.2": "purpose"
                },
                "cs-expert": "purpose",
                "bio-expert": "method"
            }
        },
        "4": {
            "segment": "CNNs are competitive on the pretraining task with transformers across several orders of magnitude in parameter size while scaling linearly with sequence length .",
            "label": {
                "basic": {
                    "B70": "finding",
                    "B105": "background",
                    "B99": "purpose",
                    "B15": "finding",
                    "B17": "method",
                    "B10": "finding",
                    "B43": "purpose",
                    "B95": "method",
                    "B18": "finding",
                    "B58": "method",
                    "B4": "finding",
                    "B98": "background",
                    "B35": "finding",
                    "B20": "finding",
                    "B47": "method",
                    "B51": "method",
                    "B107": "background",
                    "B100": "method",
                    "B2": "purpose",
                    "B104": "method"
                },
                "advanced": {
                    "A87": "purpose",
                    "A95": "purpose",
                    "A90": "method",
                    "A22": "method",
                    "A39": "finding",
                    "A56": "background",
                    "A92": "finding",
                    "A96": "other",
                    "A24": "background",
                    "A4": "finding",
                    "A91": "finding",
                    "A2": "other",
                    "A18": "method",
                    "A35": "finding",
                    "A101": "method",
                    "A7": "method",
                    "A17": "method",
                    "A6": "background",
                    "A27": "purpose",
                    "A40": "finding"
                },
                "gpt-4": {
                    "gpt-t1.0": "finding",
                    "gpt-t0.2": "finding"
                },
                "cs-expert": "background",
                "bio-expert": "finding"
            }
        },
        "5": {
            "segment": "More importantly , CNNs are competitive with and occasionally superior to transformers across an extensive set of downstream evaluations ,",
            "label": {
                "basic": {
                    "B70": "background",
                    "B105": "finding",
                    "B99": "finding",
                    "B15": "method",
                    "B17": "purpose",
                    "B10": "other",
                    "B43": "background",
                    "B95": "finding",
                    "B18": "finding",
                    "B58": "method",
                    "B4": "purpose",
                    "B98": "purpose",
                    "B35": "method",
                    "B20": "purpose",
                    "B47": "method",
                    "B51": "purpose",
                    "B107": "method",
                    "B100": "method",
                    "B2": "method",
                    "B104": "purpose"
                },
                "advanced": {
                    "A87": "purpose",
                    "A95": "background",
                    "A90": "method",
                    "A22": "purpose",
                    "A39": "purpose",
                    "A56": "background",
                    "A92": "method",
                    "A96": "background",
                    "A24": "finding",
                    "A4": "method",
                    "A91": "method",
                    "A2": "background",
                    "A18": "purpose",
                    "A35": "method",
                    "A101": "purpose",
                    "A7": "purpose",
                    "A17": "purpose",
                    "A6": "background",
                    "A27": "purpose",
                    "A40": "method"
                },
                "gpt-4": {
                    "gpt-t1.0": "finding",
                    "gpt-t0.2": "finding"
                },
                "cs-expert": "background",
                "bio-expert": "finding"
            }
        },
        "6": {
            "segment": "including structure prediction , zero-shot mutation effect prediction , and out-of-domain generalization .",
            "label": {
                "basic": {
                    "B70": "background",
                    "B105": "purpose",
                    "B99": "finding",
                    "B15": "background",
                    "B17": "finding",
                    "B10": "finding",
                    "B43": "purpose",
                    "B95": "method",
                    "B18": "finding",
                    "B58": "finding",
                    "B4": "purpose",
                    "B98": "method",
                    "B35": "finding",
                    "B20": "other",
                    "B47": "method",
                    "B51": "purpose",
                    "B107": "purpose",
                    "B100": "method",
                    "B2": "background",
                    "B104": "purpose"
                },
                "advanced": {
                    "A87": "purpose",
                    "A95": "finding",
                    "A90": "purpose",
                    "A22": "finding",
                    "A39": "purpose",
                    "A56": "purpose",
                    "A92": "finding",
                    "A96": "finding",
                    "A24": "method",
                    "A4": "purpose",
                    "A91": "finding",
                    "A2": "finding",
                    "A18": "background",
                    "A35": "other",
                    "A101": "background",
                    "A7": "background",
                    "A17": "finding",
                    "A6": "background",
                    "A27": "method",
                    "A40": "method"
                },
                "gpt-4": {
                    "gpt-t1.0": "method",
                    "gpt-t0.2": "finding"
                },
                "cs-expert": "background",
                "bio-expert": "finding"
            }
        },
        "7": {
            "segment": "We also demonstrate strong performance on sequences longer than the positional embeddings allowed in the current state-of-the-art transformer protein masked language models .",
            "label": {
                "basic": {
                    "B70": "purpose",
                    "B105": "method",
                    "B99": "finding",
                    "B15": "method",
                    "B17": "purpose",
                    "B10": "background",
                    "B43": "method",
                    "B95": "purpose",
                    "B18": "method",
                    "B58": "purpose",
                    "B4": "purpose",
                    "B98": "finding",
                    "B35": "method",
                    "B20": "finding",
                    "B47": "method",
                    "B51": "method",
                    "B107": "method",
                    "B100": "method",
                    "B2": "finding",
                    "B104": "method"
                },
                "advanced": {
                    "A87": "method",
                    "A95": "background",
                    "A90": "other",
                    "A22": "purpose",
                    "A39": "finding",
                    "A56": "method",
                    "A92": "purpose",
                    "A96": "background",
                    "A24": "purpose",
                    "A4": "method",
                    "A91": "finding",
                    "A2": "method",
                    "A18": "finding",
                    "A35": "background",
                    "A101": "purpose",
                    "A7": "method",
                    "A17": "background",
                    "A6": "background",
                    "A27": "purpose",
                    "A40": "purpose"
                },
                "gpt-4": {
                    "gpt-t1.0": "finding",
                    "gpt-t0.2": "finding"
                },
                "cs-expert": "method",
                "bio-expert": "finding"
            }
        },
        "8": {
            "segment": "Finally , we close with a call to disentangle the effects of pretraining task and model architecture when studying pretrained protein sequence models .",
            "label": {
                "basic": {
                    "B70": "purpose",
                    "B105": "background",
                    "B99": "finding",
                    "B15": "method",
                    "B17": "background",
                    "B10": "finding",
                    "B43": "finding",
                    "B95": "method",
                    "B18": "finding",
                    "B58": "finding",
                    "B4": "purpose",
                    "B98": "background",
                    "B35": "finding",
                    "B20": "finding",
                    "B47": "finding",
                    "B51": "finding",
                    "B107": "purpose",
                    "B100": "method",
                    "B2": "method",
                    "B104": "method"
                },
                "advanced": {
                    "A87": "method",
                    "A95": "background",
                    "A90": "other",
                    "A22": "finding",
                    "A39": "finding",
                    "A56": "method",
                    "A92": "background",
                    "A96": "other",
                    "A24": "purpose",
                    "A4": "finding",
                    "A91": "method",
                    "A2": "purpose",
                    "A18": "purpose",
                    "A35": "purpose",
                    "A101": "purpose",
                    "A7": "finding",
                    "A17": "purpose",
                    "A6": "background",
                    "A27": "method",
                    "A40": "method"
                },
                "gpt-4": {
                    "gpt-t1.0": "purpose",
                    "gpt-t0.2": "purpose"
                },
                "cs-expert": "finding",
                "bio-expert": "finding"
            }
        }
    }
}